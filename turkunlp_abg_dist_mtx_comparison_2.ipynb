{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch numpy gudhi -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"בחופשה האחרונה שלנו, נסענו להכיר את היופיים של מדבר הנגב. בין אם מדובר בצוקים המרשימים, בחי הבר המיוחד, או בשקט המוחלט, יש משהו מאוד מיוחד במדבר. אחת החוויות המרגשות ביותר שלנו הייתה לצפות בזרחת השמש מעל המדבר. האור המתפשט מאחורי ההרים, השמים המשתנים מאוד מהיר מאופל לתכלת, והשלווה המוחלטת שאפשר רק במדבר, הכל הפך את החוויה לבלתי נשכחת\", \n",
    "          \n",
    "          \"במהלך שנת הלימודים הראשונה שלי באוניברסיטה, הצטרפתי לקבוצת טיול שהגיעה להר האייפל. למרות הקור החודר, הייתי מחויב לעלות לפסגה בכל בוקר, כדי לצפות בזרחת השמש מעל פריס\", \n",
    "          \n",
    "          \"הייתי מתעורר בבוקר מוקדם, לפני כולם, כדי לצפות בזרחת השמש. היו ימים שהשמים היו מלאים בגוונים של ורוד וכתום, והאוויר הקר היה ממלא את הריאות. הייתי נושם את השקט, מאזין לשירת הציפורים, ומרגיש את היום החדש שמתחיל.\", \n",
    "          \n",
    "          \"הייתה לי הרגל לצפות בזרחת השמש כאשר הייתי טסה למקומות רחוקים. הייתי מתמקדת באור הזהב של השמש שהתפשט על פני האופק. זה היה רגע של שקט ושלווה, שבו הייתי מרגישה את האפשרויות של היום שלפני.\", \n",
    "          \n",
    "          \"אחד הדברים שאני ממש אוהב לעשות בחופשות הוא לצפות בזרחת השמש על גג המלון. אין דבר יותר מרגיע מאשר לשבת עם כוס קפה ביד, להתבונן בנוף, ולראות איך העולם מתעורר לחיים.\", \n",
    "          \n",
    "          \"במהלך ההליכה, עצרנו לרגע כדי לצפות בזרחת השמש. האור הראשוני של היום הזהיר את השמיים בצבעים של זהב, ואנחנו ישבנו שם בשקט, מתפללים ליום טוב.\", \n",
    "          \n",
    "          \"אני מאמין שאין דבר מרגש יותר מ לצפות בזרחת השמש. כאשר האור הראשונים מתחילים להתפשט באופק, אתה מרגיש כאילו אתה חלק ממשהו גדול מאוד. זה מזכיר לי כמה העולם הזה גדול ויפה.\", \n",
    "          \n",
    "          \"אחת הפעמים המיוחדות ביותר שבהן הזמנתי לצפות בזרחת השמש הייתה בחופשה שלי בהודו. הייתי מתעורר מוקדם, לפני כל העולם, ומשתקף למראה המרהיבה של השמש המתעלה מעל האוקיינוס. זה היה חוויה שאני לעולם לא אשכח.\", \n",
    "          \n",
    "          \"אני אוהב לצפות בזרחת השמש מהחלון שלי. זה נותן לי את האנרגיה להתחיל את היום. אני אפילו מקדיש כמה דקות בכל בוקר לקחת כוס קפה, לשבת מול החלון, ולהשתקף במראה המדהים הזה.\", \n",
    "          \n",
    "          \"בראשית, אני אוהב לצפות בזרחת השמש מהמרפסת שלי. זה מזכיר לי את היופי של העולם, את התקווה של יום חדש, ואת החיים הממשיכים להתפתח בכל יום. כל זריחה מציגה תמונה שונה, נוף חדש שממלא אותי בתחושת התרגשות והתפעלות.\", \n",
    "          \n",
    "          \"היום האחרון שלי ביפן היה יום מיוחד. יצאתי להליך מוקדם בבוקר, כדי לצפות בזרחת השמש מעל הר הפוג'. האור הראשוני של היום מאיר את השיחים המקופים בשלג, מצייר תמונה יפהפיה שאני לעולם לא אשכח.\", \n",
    "          \n",
    "          \"אחד החוויות המרגשות ביותר שלי היתה לצפות בזרחת השמש מעל הפירמידות במצרים. האור החום החודר את השחקים, מאיר את האבנים העתיקות, ומעניק להם מראה של זהב. זה היה רגע של התבוננות והתפעלות על ההיסטוריה שלנו.\", \n",
    "          \n",
    "          \"כאשר אני מטייל בים, אני מתכנן לצפות בזרחת השמש מהחוף. אין דבר מרהיב יותר מלראות את האור הראשון של היום מתפשט על גלי הים, משנה את צבעם לגוונים של זהב ואורנג'. זה הופך את החוויה של ההליכה למשהו יוצא דופן.\", \n",
    "          \n",
    "          \"האלפים היה חלום שלי. כשהגעתי לשם, הייתי עייף אבל מרוצה. לצפות בזרחת השמש מהפסגה, כשהאור העדין של השחר התחיל להתפשט על השפעי השלג הלבנים, היה חוויה בלתי נשכחת. העולם התמלא בנופים שלא ראיתי מעולם. זה היה מרגע של שלווה ושקט, שהיה שווה את כל המאמץ.\", \n",
    "          \n",
    "          \"אחרי שהוא התעורר מהשנה, איזיק הכין לעצמו כוס קפה והולך להסתובב בגן הפרטי שלו. זו הייתה הדרך האהובה עליו להתחיל את היום - לצפות בזרחת השמש ולשמוע את הציפורים מצפצפות.\", \n",
    "          \n",
    "          \"את חייבת לצפות בזרחת השמש מהחוף שלנו, אמר יגאל למרים, כאשר הם הגיעו לבית הנופש של המשפחה. זו תחוויה בלתי נשכחת, משהו שתזכורי לעוד שנים.\",\n",
    "          \n",
    "          \"בזמן שהכל בעיר עדיין ישן, רבקה מתעוררת בשעה המוקדמת ביותר שאפשר, מתארגנת, ויוצאת לרוץ. היא אוהבת את השקט של אותן שעות, והמראה של העיר שמתעוררת לחיים. אבל מעל כל, זה הזמן היחיד שהיא יכולה לצפות בזרחת השמש בלי להיות מופרעת.\",\n",
    "\n",
    "          \"לשפת האגם הגיעה מיה, המצלמה שלה כבר מוכנה לפעולה. היא התיישבה בשקט, מצפה לרגע הנכון. היא יודעת שממש בקרוב, היא תהיה מסוגלת לצפות בזרחת השמש, והיא רוצה לתפוס את הרגע המושלם בתמונה.\", \n",
    "\n",
    "          \"אחד הדברים המרגשים ביותר בנסיעה לאילת הוא ההזדמנות לצפות בזרחת השמש מעל המדבר. השמים משנים את צורתם מכל הכיוונים, מתוך כך מתמלאים בצבעים רבים, מאוד רומנטי. זהו זמן מיוחד להיות בהם, ולהרגיש את השקט שממלא את האוויר.\", \n",
    "\n",
    "          \"הלכתי לשבת על החוף, מחכה לצפות בזרחת השמש. הים היה שקט ומנוחה, האור הפליד התחיל להתפשט באופק. כשהשמש קמה, היא הפכה את השמים לפריים של צבעים מרהיבים. זה היה רגע של שלווה ושקט, שהגיע לפיקו בזריחה המרהיבה.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from transformers import AutoTokenizer, AutoModel \n",
    "import transformers\n",
    "from scipy.spatial import distance_matrix\n",
    "import gudhi as gd\n",
    "from gudhi.hera import wasserstein_distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def compute_output_model(tokenizer, model, sentence, layer, head):\n",
    "    # Load pre-trained model\n",
    "\n",
    "    # Tokenize input and convert to tensor\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass\n",
    "    # Specify `output_hidden_states=True` when calling the model\n",
    "    outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "    # Obtain the attention weights\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    # Obtain the attention weights for the specific layer and head\n",
    "    S = attentions[layer][0, head]\n",
    "\n",
    "    # Obtain the value vectors\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden_states = outputs.hidden_states[layer]\n",
    "        all_W_v = model.encoder.layer[layer].attention.self.value.weight\n",
    "        num_heads = model.config.num_attention_heads\n",
    "        head_dim = model.config.hidden_size // num_heads\n",
    "        W_v_heads = all_W_v.view(num_heads, head_dim, model.config.hidden_size)\n",
    "        W_v = W_v_heads[head]\n",
    "        V = torch.matmul(hidden_states, W_v.t())\n",
    "\n",
    "    # Compute the output O\n",
    "    O = torch.matmul(S, V)\n",
    "\n",
    "    return O\n",
    "\n",
    "\n",
    "\n",
    "def compute_phrase_distances_and_homology(tokenizer, context_vectors, sentence, phrase):\n",
    "    # Initialize the tokenizer\n",
    "\n",
    "    # Tokenize the sentence and the phrase\n",
    "    sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "    phrase_tokens = tokenizer.encode(phrase, add_special_tokens=False)\n",
    "\n",
    "    # Find the indices of the phrase tokens in the sentence\n",
    "    phrase_indices = []\n",
    "    phrase_length = len(phrase_tokens)\n",
    "    for i in range(len(sentence_tokens) - phrase_length + 1):\n",
    "        if sentence_tokens[i:i+phrase_length] == phrase_tokens:\n",
    "            phrase_indices.extend(range(i, i+phrase_length))\n",
    "            break\n",
    "\n",
    "    # Extract the context vectors for the phrase\n",
    "    phrase_context_vectors = context_vectors[0, phrase_indices]\n",
    "\n",
    "    # Detach the tensor and convert to numpy array\n",
    "    phrase_context_vectors_np = phrase_context_vectors.detach().numpy()\n",
    "\n",
    "    # Compute the pairwise Euclidean distances among the phrase context vectors\n",
    "    distances = distance_matrix(phrase_context_vectors_np, phrase_context_vectors_np)\n",
    "\n",
    "    # Compute the persistent homology of the distance matrix\n",
    "    rips_complex = gd.RipsComplex(distance_matrix=distances, max_edge_length=np.max(distances))\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=2)\n",
    "    persistent_homology = simplex_tree.persistence(min_persistence=0.001)\n",
    "\n",
    "    return persistent_homology\n",
    "\n",
    "\n",
    "\n",
    "def transform_persistence_diagram(diagram, target_dimension=0):\n",
    "    # Only include features of the target dimension and return the transformed diagram\n",
    "    return [(birth, death) for dimension, (birth, death) in diagram if dimension == target_dimension]\n",
    "\n",
    "\n",
    "\n",
    "def compute_wasserstein_distances(persistence_diagrams, p=2, target_dimension=0):\n",
    "    n = len(persistence_diagrams)\n",
    "    distances = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            diagram1 = transform_persistence_diagram(persistence_diagrams[i], target_dimension)\n",
    "            diagram2 = transform_persistence_diagram(persistence_diagrams[j], target_dimension)\n",
    "            distance = wasserstein_distance(diagram1, diagram2, order=1., internal_p=2.)\n",
    "            distances[i, j] = distance\n",
    "            distances[j, i] = distance\n",
    "    return distances\n",
    "\n",
    "\n",
    "\n",
    "def count_negative_entries_below_diagonal(matrix):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    n = len(matrix)\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            if matrix[i][j] < 0:\n",
    "                count += 1\n",
    "            total += 1\n",
    "    return count, total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the context vectors for all layers and heads for the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"TurkuNLP/wikibert-base-he-cased\")\n",
    "model = transformers.BertModel.from_pretrained(\"TurkuNLP/wikibert-base-he-cased\", output_attentions=True)\n",
    "\n",
    "# Get the number of layers and heads in the model\n",
    "num_layers = model.config.num_hidden_layers\n",
    "num_heads = model.config.num_attention_heads\n",
    "\n",
    "context = []\n",
    "for i in range(len(text)-13):\n",
    "    sentence_context = []\n",
    "    for layer in range(num_layers):\n",
    "        layer_context = []\n",
    "        for head in range(num_heads):\n",
    "            layer_context.append(compute_output_model(tokenizer, model, text[i], layer, head))\n",
    "        sentence_context.append(layer_context)\n",
    "    context.append(sentence_context)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the context vectors for the first model for `paragraph`, `layer`, and `head`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48, 64])\n",
      "tensor([[[-0.2458, -0.1798,  0.2407,  ..., -0.1119, -0.1072,  0.1996],\n",
      "         [-0.1628, -0.1793,  0.3843,  ..., -0.1719,  0.1051,  0.4308],\n",
      "         [-1.1078, -0.3946,  0.6971,  ..., -0.1880,  0.1257,  0.0926],\n",
      "         ...,\n",
      "         [-0.1866,  0.1493, -0.2303,  ...,  0.2310, -0.1824, -0.0043],\n",
      "         [-0.2282, -0.3970, -0.2086,  ..., -0.2019,  0.2671,  0.1893],\n",
      "         [ 0.0132, -0.0570,  0.1171,  ..., -0.0445, -0.0106,  0.1866]]],\n",
      "       grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "paragraph = len(text)-14\n",
    "layer = 11\n",
    "head = 11\n",
    "\n",
    "print(context[paragraph][layer][head].shape)\n",
    "print(context[paragraph][layer][head])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the context vectors for all layers and heads for the second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dicta-il/alephbertgimmel-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dicta-il/alephbertgimmel-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_2 = AutoTokenizer.from_pretrained('dicta-il/alephbertgimmel-base')\n",
    "model_2 = AutoModel.from_pretrained(\"dicta-il/alephbertgimmel-base\", output_attentions=True) \n",
    "\n",
    "# Get the number of layers and heads in the model\n",
    "num_layers = model_2.config.num_hidden_layers\n",
    "num_heads = model_2.config.num_attention_heads\n",
    "\n",
    "context_2 = []\n",
    "for i in range(len(text)-13):\n",
    "    sentence_context_2 = []\n",
    "    for layer in range(num_layers):\n",
    "        layer_context_2 = []\n",
    "        for head in range(num_heads):\n",
    "            layer_context_2.append(compute_output_model(tokenizer_2, model_2, text[i], layer, head))\n",
    "        sentence_context_2.append(layer_context_2)\n",
    "    context_2.append(sentence_context_2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the context vectors for the second model for `paragraph`, `layer`, and `head`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 39, 64])\n",
      "tensor([[[ 0.0121,  0.0056,  0.0036,  ..., -0.0106,  0.0054,  0.0007],\n",
      "         [ 0.0053,  0.0444, -0.0070,  ..., -0.0396, -0.0418, -0.0150],\n",
      "         [ 0.0139,  0.0409,  0.0100,  ..., -0.0144, -0.0257,  0.0045],\n",
      "         ...,\n",
      "         [ 0.0423,  0.0082,  0.0512,  ..., -0.0332,  0.0426, -0.0145],\n",
      "         [ 0.0227,  0.0100,  0.0133,  ..., -0.0279,  0.0040, -0.0100],\n",
      "         [ 0.0215,  0.0009,  0.0107,  ..., -0.0203,  0.0101, -0.0038]]],\n",
      "       grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "paragraph = len(text)-14\n",
    "layer = 11\n",
    "head = 11\n",
    "\n",
    "print(context_2[paragraph][layer][head].shape)\n",
    "print(context_2[paragraph][layer][head])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Persistent Homology of the Keyphrase for the First Model Across All Contexts\n",
    "\n",
    "Here we compute the persistent homoloy of the keyphrase \"לצפות בזרחת השמש\" for each context paragraph `text[i]`, for each head of each layer. This will give us one persistence diagram per context, per head of the model. We store the results in `persistent_homology`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"TurkuNLP/wikibert-base-he-cased\")\n",
    "\n",
    "# Get the number of layers and heads in the model\n",
    "num_layers = model.config.num_hidden_layers\n",
    "num_heads = model.config.num_attention_heads\n",
    "\n",
    "persistent_homology = []\n",
    "\n",
    "for i in range(len(text)-13):\n",
    "    sentence_persistent_homology = []\n",
    "    for layer in range(num_layers):\n",
    "        layer_persistent_homology = []\n",
    "        for head in range(num_heads):\n",
    "            layer_persistent_homology.append(compute_phrase_distances_and_homology(tokenizer, context[i][layer][head], text[i], \"לצפות בזרחת השמש\"))\n",
    "        sentence_persistent_homology.append(layer_persistent_homology)\n",
    "    persistent_homology.append(sentence_persistent_homology)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (0.0, inf)),\n",
       " (0, (0.0, 3.803238669101408)),\n",
       " (0, (0.0, 2.9890673462818973)),\n",
       " (0, (0.0, 2.833318585966194)),\n",
       " (0, (0.0, 2.6633386924894653))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = len(text)-14\n",
    "layer = 11\n",
    "head = 11\n",
    "\n",
    "persistent_homology[paragraph][layer][head]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Persistent Homology of the Keyphrase for the Second Model Across All Contexts\n",
    "\n",
    "Here we compute the persistent homoloy of the keyphrase \"לצפות בזרחת השמש\" for each context paragraph `text[i]`, for each head of each layer. This will give us one persistence diagram per context, per head of the model. We store the results in `persistent_homology_2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_2 = AutoTokenizer.from_pretrained('dicta-il/alephbertgimmel-base')\n",
    "\n",
    "# Get the number of layers and heads in the model\n",
    "num_layers = model_2.config.num_hidden_layers\n",
    "num_heads = model_2.config.num_attention_heads\n",
    "\n",
    "persistent_homology_2 = []\n",
    "\n",
    "for i in range(len(text)-13):\n",
    "    sentence_persistent_homology_2 = []\n",
    "    for layer in range(num_layers):\n",
    "        layer_persistent_homology_2 = []\n",
    "        for head in range(num_heads):\n",
    "            layer_persistent_homology_2.append(compute_phrase_distances_and_homology(tokenizer_2, context_2[i][layer][head], text[i], \"לצפות בזרחת השמש\"))\n",
    "        sentence_persistent_homology_2.append(layer_persistent_homology_2)\n",
    "    persistent_homology_2.append(sentence_persistent_homology_2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Persistence Diagrams for First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of layers and heads in the model\n",
    "num_layers = model.config.num_hidden_layers\n",
    "num_heads = model.config.num_attention_heads\n",
    "\n",
    "persistence_diagrams = []\n",
    "\n",
    "for i in range(len(text)-13):\n",
    "    sentence_persistence_diagrams = []\n",
    "    for layer in range(num_layers):\n",
    "        layer_persistence_diagrams = []\n",
    "        for head in range(num_heads):\n",
    "            layer_persistence_diagrams.append(persistent_homology[i][layer][head])\n",
    "        sentence_persistence_diagrams.append(layer_persistence_diagrams)\n",
    "    persistence_diagrams.append(sentence_persistence_diagrams)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Persistence Diagrams for Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of layers and heads in the model\n",
    "num_layers = model_2.config.num_hidden_layers\n",
    "num_heads = model_2.config.num_attention_heads\n",
    "\n",
    "persistence_diagrams_2 = []\n",
    "\n",
    "for i in range(len(text)-13):\n",
    "    sentence_persistence_diagrams_2 = []\n",
    "    for layer in range(num_layers):\n",
    "        layer_persistence_diagrams_2 = []\n",
    "        for head in range(num_heads):\n",
    "            layer_persistence_diagrams_2.append(persistent_homology_2[i][layer][head])\n",
    "        sentence_persistence_diagrams_2.append(layer_persistence_diagrams_2)\n",
    "    persistence_diagrams_2.append(sentence_persistence_diagrams_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Matrices for `layer` and `head` of the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of layers and heads in the model\n",
    "num_layers = model.config.num_hidden_layers\n",
    "num_heads = model.config.num_attention_heads\n",
    "\n",
    "# Initialize w_distances\n",
    "w_distances = [[None for _ in range(num_heads)] for _ in range(num_layers)]\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    for head in range(num_heads):\n",
    "        w_distances[layer][head] = compute_wasserstein_distances([persistent_homology[paragraph][layer][head] for paragraph in range(len(text)-13)])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we print the Wasserstein distance matrix giving the pairwise Wasserstein distances between each pair of persistence diagrams for the phrase \"לצפות בזרחת השמש\" in each of the contexts `text[i]` (we have restricted to a particular range of `i` values due to computational limitations). Note here, we have a $7 \\times 7$ distance matrix because we have restricted to `len(text)-13` contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7)\n",
      "[[0.         1.91550692 5.42725774 3.28734218 6.43482884 8.64706343\n",
      "  0.56759603]\n",
      " [1.91550692 0.         3.51175082 1.82834971 4.51932193 6.73155651\n",
      "  1.5390758 ]\n",
      " [5.42725774 3.51175082 0.         2.24399684 1.0075711  3.21980569\n",
      "  5.05082662]\n",
      " [3.28734218 1.82834971 2.24399684 0.         3.14748666 5.35972125\n",
      "  2.91091106]\n",
      " [6.43482884 4.51932193 1.0075711  3.14748666 0.         2.21223459\n",
      "  6.05839772]\n",
      " [8.64706343 6.73155651 3.21980569 5.35972125 2.21223459 0.\n",
      "  8.27063231]\n",
      " [0.56759603 1.5390758  5.05082662 2.91091106 6.05839772 8.27063231\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(w_distances[layer][head].shape)\n",
    "print(w_distances[layer][head])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Matrices for `layer` and `head` of the second model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of layers and heads in the model\n",
    "num_layers = model_2.config.num_hidden_layers\n",
    "num_heads = model_2.config.num_attention_heads\n",
    "\n",
    "# Initialize w_distances\n",
    "w_distances_2 = [[None for _ in range(num_heads)] for _ in range(num_layers)]\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    for head in range(num_heads):\n",
    "        w_distances_2[layer][head] = compute_wasserstein_distances([persistent_homology_2[paragraph][layer][head] for paragraph in range(len(text)-13)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7)\n",
      "[[0.         0.41156989 0.30807921 0.53128823 0.3306788  0.52056734\n",
      "  0.18950863]\n",
      " [0.41156989 0.         0.63907578 0.94285812 0.47170782 0.93213723\n",
      "  0.53128737]\n",
      " [0.30807921 0.63907578 0.         0.30378234 0.5869847  0.29306145\n",
      "  0.25166465]\n",
      " [0.53128823 0.94285812 0.30378234 0.         0.78024975 0.06063153\n",
      "  0.41157075]\n",
      " [0.3306788  0.47170782 0.5869847  0.78024975 0.         0.76952886\n",
      "  0.368679  ]\n",
      " [0.52056734 0.93213723 0.29306145 0.06063153 0.76952886 0.\n",
      "  0.4112754 ]\n",
      " [0.18950863 0.53128737 0.25166465 0.41157075 0.368679   0.4112754\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(w_distances_2[layer][head].shape)\n",
    "print(w_distances_2[layer][head])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentages for Performance Comparison\n",
    "\n",
    "This next block of code will create a $144 \\times 144$ array (or more accurately a $12 \\times 12 \\times 12 \\times 12$ array, since there are $12$ layers with $12$ heads in each layer) to store the percentages indication what percentage of entries of the matrices `w_distances[layer_1][head_1] - w_distances_2[layer_2][head_2]` are negative. Here, we are comparing each head's Wasserstein distances in one model, that is `w_distances[layer_1][head_1]` to each head's Wasserstein distances in a second model, that is `w_distances_2[layer_2][head_2]`. This effectively compares how well the persistent homology is preserved by one `head` in the first model, to how well the persistent homology is preserved by a second `head` in a second model. If the percentage of negative entries is high in the matrix `w_distances[layer_1][head_1] - w_distances_2[layer_2][head_2]`, then the matrix `w_distances_2[layer_2][head_2]` is larger in more places than not, indcating that the second model's Wasserstein distances are higher much more often than the first model's Wasserstein distances. This means the second model is *worse* at preserving persistent homology. On the other hand, if the percentage of the entries in `w_distances[layer_1][head_1] - w_distances_2[layer_2][head_2]` that are negative is low, then the first model's attention head outperforms the second model's attention head at preserving the persistent homology. Doing this for every `head` in every`layer`, w get the $12 \\times 12 \\times 12 \\times 12$ array `percentages`. Querying `percentages` by calling `percentages[layer_1][head_1][layer_2][head_2]` for a heads in specific layers gives us the percentage of entries below the diagonal in `w_distances[layer_1][head_1] - w_distances_2[layer_2][head_2]` that are negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 12, 12, 12)\n",
      "Percentages of negative entries below the diagonal:  [[[[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [14.28571429  0.          0.         ...  0.          0.\n",
      "     9.52380952]]\n",
      "\n",
      "  [[ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [14.28571429  0.          0.         ...  4.76190476  0.\n",
      "     4.76190476]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     4.76190476]\n",
      "   ...\n",
      "   [14.28571429  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [38.0952381   0.          0.         ...  0.          0.\n",
      "    28.57142857]]\n",
      "\n",
      "  [[ 4.76190476  4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     4.76190476]\n",
      "   ...\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [19.04761905  0.          0.         ...  0.          0.\n",
      "     4.76190476]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 9.52380952  9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [47.61904762  4.76190476  0.         ...  4.76190476 23.80952381\n",
      "    28.57142857]\n",
      "   [14.28571429  4.76190476  4.76190476 ...  4.76190476  0.\n",
      "     9.52380952]\n",
      "   ...\n",
      "   [28.57142857  9.52380952  9.52380952 ...  0.         14.28571429\n",
      "     4.76190476]\n",
      "   [ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [71.42857143  0.          0.         ...  9.52380952  0.\n",
      "    52.38095238]]\n",
      "\n",
      "  [[ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          9.52380952\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [33.33333333  0.          0.         ...  0.          0.\n",
      "    23.80952381]]\n",
      "\n",
      "  [[ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [28.57142857  0.          0.         ...  0.         14.28571429\n",
      "     9.52380952]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     4.76190476]\n",
      "   ...\n",
      "   [ 9.52380952  4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [52.38095238  0.          0.         ...  4.76190476  0.\n",
      "    33.33333333]]]\n",
      "\n",
      "\n",
      " [[[ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [19.04761905  0.          0.         ...  0.         14.28571429\n",
      "     4.76190476]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 4.76190476  4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [52.38095238  0.          0.         ...  0.          0.\n",
      "    19.04761905]]\n",
      "\n",
      "  [[ 0.         14.28571429  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 4.76190476  9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [33.33333333  0.          0.         ...  0.          0.\n",
      "    14.28571429]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 9.52380952 28.57142857  0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [42.85714286  0.          0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [23.80952381  4.76190476  9.52380952 ...  0.          0.\n",
      "    14.28571429]\n",
      "   ...\n",
      "   [28.57142857 33.33333333  9.52380952 ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [ 0.         14.28571429  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [57.14285714  0.          0.         ... 14.28571429  0.\n",
      "    47.61904762]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          9.52380952\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 4.76190476  4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [33.33333333  0.          0.         ...  0.          0.\n",
      "    19.04761905]]\n",
      "\n",
      "  [[ 0.         28.57142857  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [42.85714286  0.          0.         ...  0.         19.04761905\n",
      "    19.04761905]\n",
      "   [14.28571429  0.          4.76190476 ...  0.          4.76190476\n",
      "    23.80952381]\n",
      "   ...\n",
      "   [28.57142857 38.0952381   4.76190476 ...  0.         14.28571429\n",
      "     4.76190476]\n",
      "   [ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [57.14285714  0.          0.         ...  9.52380952  0.\n",
      "    57.14285714]]]\n",
      "\n",
      "\n",
      " [[[ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [14.28571429  0.          0.         ...  0.          9.52380952\n",
      "     9.52380952]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [23.80952381  0.          0.         ...  0.          0.\n",
      "     4.76190476]]\n",
      "\n",
      "  [[ 0.         38.0952381   0.         ...  0.          0.\n",
      "     4.76190476]\n",
      "   [42.85714286  0.          0.         ...  0.         23.80952381\n",
      "     9.52380952]\n",
      "   [ 4.76190476  4.76190476  0.         ...  0.          0.\n",
      "     9.52380952]\n",
      "   ...\n",
      "   [19.04761905 23.80952381  0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [61.9047619   0.          0.         ...  4.76190476  0.\n",
      "    47.61904762]]\n",
      "\n",
      "  [[ 4.76190476 47.61904762  0.         ...  0.          4.76190476\n",
      "     4.76190476]\n",
      "   [57.14285714  0.          0.         ...  0.         33.33333333\n",
      "    23.80952381]\n",
      "   [19.04761905  9.52380952  4.76190476 ...  0.          0.\n",
      "    14.28571429]\n",
      "   ...\n",
      "   [28.57142857 33.33333333  0.         ...  0.          9.52380952\n",
      "     4.76190476]\n",
      "   [ 4.76190476  4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [71.42857143  0.          0.         ...  4.76190476  0.\n",
      "    66.66666667]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.         33.33333333  0.         ...  4.76190476  4.76190476\n",
      "     0.        ]\n",
      "   [42.85714286  4.76190476  0.         ...  0.         33.33333333\n",
      "    19.04761905]\n",
      "   [ 9.52380952  4.76190476  4.76190476 ...  0.          0.\n",
      "     4.76190476]\n",
      "   ...\n",
      "   [28.57142857 23.80952381  9.52380952 ...  0.          0.\n",
      "     4.76190476]\n",
      "   [ 4.76190476  4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [66.66666667  0.          0.         ...  4.76190476  0.\n",
      "    52.38095238]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [19.04761905  0.          0.         ...  0.          0.\n",
      "     4.76190476]]\n",
      "\n",
      "  [[33.33333333 42.85714286  4.76190476 ...  4.76190476 14.28571429\n",
      "    14.28571429]\n",
      "   [47.61904762  9.52380952 14.28571429 ... 14.28571429 47.61904762\n",
      "    33.33333333]\n",
      "   [52.38095238 19.04761905 23.80952381 ... 23.80952381 28.57142857\n",
      "    42.85714286]\n",
      "   ...\n",
      "   [47.61904762 33.33333333 19.04761905 ... 19.04761905 33.33333333\n",
      "    19.04761905]\n",
      "   [14.28571429 33.33333333  4.76190476 ...  0.          0.\n",
      "     0.        ]\n",
      "   [76.19047619  0.          0.         ... 33.33333333  0.\n",
      "    66.66666667]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [14.28571429  0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [14.28571429  0.          0.         ...  0.          0.\n",
      "     4.76190476]]\n",
      "\n",
      "  [[ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 4.76190476  4.76190476  4.76190476 ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [23.80952381  0.          0.         ...  0.          0.\n",
      "     4.76190476]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     4.76190476]]\n",
      "\n",
      "  [[ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     4.76190476]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [19.04761905  0.          0.         ...  0.          0.\n",
      "     4.76190476]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     9.52380952]]]\n",
      "\n",
      "\n",
      " [[[ 0.         23.80952381  4.76190476 ...  4.76190476  4.76190476\n",
      "     4.76190476]\n",
      "   [14.28571429  4.76190476  0.         ...  4.76190476 14.28571429\n",
      "     9.52380952]\n",
      "   [19.04761905  4.76190476  9.52380952 ...  0.          0.\n",
      "     4.76190476]\n",
      "   ...\n",
      "   [23.80952381 14.28571429  9.52380952 ...  0.          4.76190476\n",
      "     4.76190476]\n",
      "   [ 9.52380952  9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [52.38095238  4.76190476  0.         ...  9.52380952  0.\n",
      "    33.33333333]]\n",
      "\n",
      "  [[ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          9.52380952\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.         14.28571429  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [23.80952381  0.          0.         ...  0.          0.\n",
      "    19.04761905]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [23.80952381  0.          0.         ...  0.          4.76190476\n",
      "     4.76190476]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 9.52380952  4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [42.85714286  0.          0.         ...  0.          0.\n",
      "    14.28571429]]\n",
      "\n",
      "  [[19.04761905 47.61904762 19.04761905 ...  4.76190476 23.80952381\n",
      "    19.04761905]\n",
      "   [47.61904762 14.28571429  9.52380952 ... 14.28571429 28.57142857\n",
      "    19.04761905]\n",
      "   [52.38095238 19.04761905 23.80952381 ...  9.52380952  4.76190476\n",
      "    28.57142857]\n",
      "   ...\n",
      "   [47.61904762 38.0952381  14.28571429 ...  4.76190476 33.33333333\n",
      "    14.28571429]\n",
      "   [19.04761905 28.57142857  4.76190476 ...  4.76190476  0.\n",
      "     0.        ]\n",
      "   [66.66666667  9.52380952  0.         ... 28.57142857  0.\n",
      "    61.9047619 ]]\n",
      "\n",
      "  [[ 9.52380952 14.28571429  4.76190476 ...  0.          4.76190476\n",
      "     9.52380952]\n",
      "   [23.80952381  4.76190476  0.         ...  0.         19.04761905\n",
      "     0.        ]\n",
      "   [23.80952381  9.52380952 14.28571429 ...  0.          0.\n",
      "    14.28571429]\n",
      "   ...\n",
      "   [19.04761905  9.52380952  9.52380952 ...  0.          9.52380952\n",
      "     0.        ]\n",
      "   [ 9.52380952 14.28571429  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [38.0952381   0.          0.         ...  0.          0.\n",
      "    28.57142857]]]\n",
      "\n",
      "\n",
      " [[[ 0.         42.85714286  0.         ...  0.          4.76190476\n",
      "     4.76190476]\n",
      "   [33.33333333  4.76190476  0.         ...  9.52380952 19.04761905\n",
      "     9.52380952]\n",
      "   [14.28571429  0.          9.52380952 ...  4.76190476  0.\n",
      "     4.76190476]\n",
      "   ...\n",
      "   [28.57142857 28.57142857  0.         ...  0.          9.52380952\n",
      "     4.76190476]\n",
      "   [ 4.76190476 14.28571429  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [47.61904762  0.          0.         ...  9.52380952  0.\n",
      "    38.0952381 ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     4.76190476]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[23.80952381 57.14285714 14.28571429 ...  4.76190476 19.04761905\n",
      "    14.28571429]\n",
      "   [71.42857143 19.04761905 14.28571429 ... 19.04761905 28.57142857\n",
      "    23.80952381]\n",
      "   [38.0952381  14.28571429 33.33333333 ... 23.80952381  4.76190476\n",
      "    28.57142857]\n",
      "   ...\n",
      "   [57.14285714 42.85714286 19.04761905 ...  0.         33.33333333\n",
      "    28.57142857]\n",
      "   [28.57142857 42.85714286  0.         ...  9.52380952  0.\n",
      "     0.        ]\n",
      "   [57.14285714  0.          0.         ... 23.80952381  0.\n",
      "    61.9047619 ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [14.28571429  0.          0.         ...  0.          0.\n",
      "     9.52380952]]\n",
      "\n",
      "  [[ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     4.76190476]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     0.        ]]]]\n"
     ]
    }
   ],
   "source": [
    "# Create a 4D array to store the percentages\n",
    "percentages = np.zeros((num_layers, num_heads, num_layers, num_heads))\n",
    "\n",
    "# Loop over all pairs of layers and heads\n",
    "for layer_1 in range(num_layers):\n",
    "    for head_1 in range(num_heads):\n",
    "        for layer_2 in range(num_layers):\n",
    "            for head_2 in range(num_heads):\n",
    "                # Compute the difference of their Wasserstein distance matrices\n",
    "                matrix = w_distances[layer_1][head_1] - w_distances_2[layer_2][head_2]\n",
    "\n",
    "                # Count the percentage of negative entries below the diagonal\n",
    "                negative_count, total_count = count_negative_entries_below_diagonal(matrix)\n",
    "                percentage = (negative_count / total_count) * 100\n",
    "\n",
    "                # Store the percentage\n",
    "                percentages[layer_1, head_1, layer_2, head_2] = percentage\n",
    "\n",
    "print(percentages.shape)\n",
    "print(\"Percentages of negative entries below the diagonal: \", percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.523809523809524\n"
     ]
    }
   ],
   "source": [
    "layer_1 = 11\n",
    "head_1 = 5\n",
    "\n",
    "layer_2 = 1\n",
    "head_2 = 1\n",
    "\n",
    "print(percentages[layer_1][head_1][layer_2][head_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  0.0\n",
      "Max:  90.47619047619048\n",
      "Mean:  4.079172178130511\n",
      "Median:  0.0\n",
      "Standard Deviation:  10.065995771649103\n",
      "25th percentile:  0.0\n",
      "75th percentile:  4.761904761904762\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a list to store all entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over all entries in the 4D array\n",
    "for layer_1 in range(num_layers):\n",
    "    for head_1 in range(num_heads):\n",
    "        for layer_2 in range(num_layers):\n",
    "            for head_2 in range(num_heads):\n",
    "                # Add the entry to the list\n",
    "                all_entries.append(percentages[layer_1, head_1, layer_2, head_2])\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "all_entries = np.array(all_entries)\n",
    "\n",
    "# Compute the statistics\n",
    "min_value = np.min(all_entries)\n",
    "max_value = np.max(all_entries)\n",
    "mean_value = np.mean(all_entries)\n",
    "median_value = np.median(all_entries)\n",
    "std_dev = np.std(all_entries)\n",
    "percentile_25 = np.percentile(all_entries, 25)\n",
    "percentile_75 = np.percentile(all_entries, 75)\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Min: \", min_value)\n",
    "print(\"Max: \", max_value)\n",
    "print(\"Mean: \", mean_value)\n",
    "print(\"Median: \", median_value)\n",
    "print(\"Standard Deviation: \", std_dev)\n",
    "print(\"25th percentile: \", percentile_25)\n",
    "print(\"75th percentile: \", percentile_75)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can tell that a large majority of the time the second model `dicta-il/alephbertgimmel-base` outperforms the first model `TurkuNLP/wikibert-base-he-cased` at preserving the persistent homology of the phrase \"לצפות בזרחת השמש\" in the various contexts given by each `text[i]`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
