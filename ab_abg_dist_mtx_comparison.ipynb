{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch numpy gudhi -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"בחופשה האחרונה שלנו, נסענו להכיר את היופיים של מדבר הנגב. בין אם מדובר בצוקים המרשימים, בחי הבר המיוחד, או בשקט המוחלט, יש משהו מאוד מיוחד במדבר. אחת החוויות המרגשות ביותר שלנו הייתה לצפות בזרחת השמש מעל המדבר. האור המתפשט מאחורי ההרים, השמים המשתנים מאוד מהיר מאופל לתכלת, והשלווה המוחלטת שאפשר רק במדבר, הכל הפך את החוויה לבלתי נשכחת\", \n",
    "          \n",
    "          \"במהלך שנת הלימודים הראשונה שלי באוניברסיטה, הצטרפתי לקבוצת טיול שהגיעה להר האייפל. למרות הקור החודר, הייתי מחויב לעלות לפסגה בכל בוקר, כדי לצפות בזרחת השמש מעל פריס\", \n",
    "          \n",
    "          \"הייתי מתעורר בבוקר מוקדם, לפני כולם, כדי לצפות בזרחת השמש. היו ימים שהשמים היו מלאים בגוונים של ורוד וכתום, והאוויר הקר היה ממלא את הריאות. הייתי נושם את השקט, מאזין לשירת הציפורים, ומרגיש את היום החדש שמתחיל.\", \n",
    "          \n",
    "          \"הייתה לי הרגל לצפות בזרחת השמש כאשר הייתי טסה למקומות רחוקים. הייתי מתמקדת באור הזהב של השמש שהתפשט על פני האופק. זה היה רגע של שקט ושלווה, שבו הייתי מרגישה את האפשרויות של היום שלפני.\", \n",
    "          \n",
    "          \"אחד הדברים שאני ממש אוהב לעשות בחופשות הוא לצפות בזרחת השמש על גג המלון. אין דבר יותר מרגיע מאשר לשבת עם כוס קפה ביד, להתבונן בנוף, ולראות איך העולם מתעורר לחיים.\", \n",
    "          \n",
    "          \"במהלך ההליכה, עצרנו לרגע כדי לצפות בזרחת השמש. האור הראשוני של היום הזהיר את השמיים בצבעים של זהב, ואנחנו ישבנו שם בשקט, מתפללים ליום טוב.\", \n",
    "          \n",
    "          \"אני מאמין שאין דבר מרגש יותר מ לצפות בזרחת השמש. כאשר האור הראשונים מתחילים להתפשט באופק, אתה מרגיש כאילו אתה חלק ממשהו גדול מאוד. זה מזכיר לי כמה העולם הזה גדול ויפה.\", \n",
    "          \n",
    "          \"אחת הפעמים המיוחדות ביותר שבהן הזמנתי לצפות בזרחת השמש הייתה בחופשה שלי בהודו. הייתי מתעורר מוקדם, לפני כל העולם, ומשתקף למראה המרהיבה של השמש המתעלה מעל האוקיינוס. זה היה חוויה שאני לעולם לא אשכח.\", \n",
    "          \n",
    "          \"אני אוהב לצפות בזרחת השמש מהחלון שלי. זה נותן לי את האנרגיה להתחיל את היום. אני אפילו מקדיש כמה דקות בכל בוקר לקחת כוס קפה, לשבת מול החלון, ולהשתקף במראה המדהים הזה.\", \n",
    "          \n",
    "          \"בראשית, אני אוהב לצפות בזרחת השמש מהמרפסת שלי. זה מזכיר לי את היופי של העולם, את התקווה של יום חדש, ואת החיים הממשיכים להתפתח בכל יום. כל זריחה מציגה תמונה שונה, נוף חדש שממלא אותי בתחושת התרגשות והתפעלות.\", \n",
    "          \n",
    "          \"היום האחרון שלי ביפן היה יום מיוחד. יצאתי להליך מוקדם בבוקר, כדי לצפות בזרחת השמש מעל הר הפוג'. האור הראשוני של היום מאיר את השיחים המקופים בשלג, מצייר תמונה יפהפיה שאני לעולם לא אשכח.\", \n",
    "          \n",
    "          \"אחד החוויות המרגשות ביותר שלי היתה לצפות בזרחת השמש מעל הפירמידות במצרים. האור החום החודר את השחקים, מאיר את האבנים העתיקות, ומעניק להם מראה של זהב. זה היה רגע של התבוננות והתפעלות על ההיסטוריה שלנו.\", \n",
    "          \n",
    "          \"כאשר אני מטייל בים, אני מתכנן לצפות בזרחת השמש מהחוף. אין דבר מרהיב יותר מלראות את האור הראשון של היום מתפשט על גלי הים, משנה את צבעם לגוונים של זהב ואורנג'. זה הופך את החוויה של ההליכה למשהו יוצא דופן.\", \n",
    "          \n",
    "          \"האלפים היה חלום שלי. כשהגעתי לשם, הייתי עייף אבל מרוצה. לצפות בזרחת השמש מהפסגה, כשהאור העדין של השחר התחיל להתפשט על השפעי השלג הלבנים, היה חוויה בלתי נשכחת. העולם התמלא בנופים שלא ראיתי מעולם. זה היה מרגע של שלווה ושקט, שהיה שווה את כל המאמץ.\", \n",
    "          \n",
    "          \"אחרי שהוא התעורר מהשנה, איזיק הכין לעצמו כוס קפה והולך להסתובב בגן הפרטי שלו. זו הייתה הדרך האהובה עליו להתחיל את היום - לצפות בזרחת השמש ולשמוע את הציפורים מצפצפות.\", \n",
    "          \n",
    "          \"את חייבת לצפות בזרחת השמש מהחוף שלנו, אמר יגאל למרים, כאשר הם הגיעו לבית הנופש של המשפחה. זו תחוויה בלתי נשכחת, משהו שתזכורי לעוד שנים.\",\n",
    "          \n",
    "          \"בזמן שהכל בעיר עדיין ישן, רבקה מתעוררת בשעה המוקדמת ביותר שאפשר, מתארגנת, ויוצאת לרוץ. היא אוהבת את השקט של אותן שעות, והמראה של העיר שמתעוררת לחיים. אבל מעל כל, זה הזמן היחיד שהיא יכולה לצפות בזרחת השמש בלי להיות מופרעת.\",\n",
    "\n",
    "          \"לשפת האגם הגיעה מיה, המצלמה שלה כבר מוכנה לפעולה. היא התיישבה בשקט, מצפה לרגע הנכון. היא יודעת שממש בקרוב, היא תהיה מסוגלת לצפות בזרחת השמש, והיא רוצה לתפוס את הרגע המושלם בתמונה.\", \n",
    "\n",
    "          \"אחד הדברים המרגשים ביותר בנסיעה לאילת הוא ההזדמנות לצפות בזרחת השמש מעל המדבר. השמים משנים את צורתם מכל הכיוונים, מתוך כך מתמלאים בצבעים רבים, מאוד רומנטי. זהו זמן מיוחד להיות בהם, ולהרגיש את השקט שממלא את האוויר.\", \n",
    "\n",
    "          \"הלכתי לשבת על החוף, מחכה לצפות בזרחת השמש. הים היה שקט ומנוחה, האור הפליד התחיל להתפשט באופק. כשהשמש קמה, היא הפכה את השמים לפריים של צבעים מרהיבים. זה היה רגע של שלווה ושקט, שהגיע לפיקו בזריחה המרהיבה.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from transformers import AutoTokenizer, AutoModel \n",
    "import transformers\n",
    "from scipy.spatial import distance_matrix\n",
    "import gudhi as gd\n",
    "from gudhi.hera import wasserstein_distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def compute_output_model(tokenizer, model, sentence, layer, head):\n",
    "    # Load pre-trained model\n",
    "\n",
    "    # Tokenize input and convert to tensor\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass\n",
    "    # Specify `output_hidden_states=True` when calling the model\n",
    "    outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "    # Obtain the attention weights\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    # Obtain the attention weights for the specific layer and head\n",
    "    S = attentions[layer][0, head]\n",
    "\n",
    "    # Obtain the value vectors\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden_states = outputs.hidden_states[layer]\n",
    "        all_W_v = model.encoder.layer[layer].attention.self.value.weight\n",
    "        num_heads = model.config.num_attention_heads\n",
    "        head_dim = model.config.hidden_size // num_heads\n",
    "        W_v_heads = all_W_v.view(num_heads, head_dim, model.config.hidden_size)\n",
    "        W_v = W_v_heads[head]\n",
    "        V = torch.matmul(hidden_states, W_v.t())\n",
    "\n",
    "    # Compute the output O\n",
    "    O = torch.matmul(S, V)\n",
    "\n",
    "    return O\n",
    "\n",
    "\n",
    "\n",
    "def compute_phrase_distances_and_homology(tokenizer, context_vectors, sentence, phrase):\n",
    "    # Initialize the tokenizer\n",
    "\n",
    "    # Tokenize the sentence and the phrase\n",
    "    sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "    phrase_tokens = tokenizer.encode(phrase, add_special_tokens=False)\n",
    "\n",
    "    # Find the indices of the phrase tokens in the sentence\n",
    "    phrase_indices = []\n",
    "    phrase_length = len(phrase_tokens)\n",
    "    for i in range(len(sentence_tokens) - phrase_length + 1):\n",
    "        if sentence_tokens[i:i+phrase_length] == phrase_tokens:\n",
    "            phrase_indices.extend(range(i, i+phrase_length))\n",
    "            break\n",
    "\n",
    "    # Extract the context vectors for the phrase\n",
    "    phrase_context_vectors = context_vectors[0, phrase_indices]\n",
    "\n",
    "    # Detach the tensor and convert to numpy array\n",
    "    phrase_context_vectors_np = phrase_context_vectors.detach().numpy()\n",
    "\n",
    "    # Compute the pairwise Euclidean distances among the phrase context vectors\n",
    "    distances = distance_matrix(phrase_context_vectors_np, phrase_context_vectors_np)\n",
    "\n",
    "    # Compute the persistent homology of the distance matrix\n",
    "    rips_complex = gd.RipsComplex(distance_matrix=distances, max_edge_length=np.max(distances))\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=2)\n",
    "    persistent_homology = simplex_tree.persistence(min_persistence=0.001)\n",
    "\n",
    "    return persistent_homology\n",
    "\n",
    "\n",
    "\n",
    "def transform_persistence_diagram(diagram):\n",
    "    # Remove the dimension from each feature and return the transformed diagram\n",
    "    return [(birth, death) for dimension, (birth, death) in diagram]\n",
    "\n",
    "\n",
    "\n",
    "def compute_wasserstein_distances(persistence_diagrams, p=2):\n",
    "    n = len(persistence_diagrams)\n",
    "    distances = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            diagram1 = transform_persistence_diagram(persistence_diagrams[i])\n",
    "            diagram2 = transform_persistence_diagram(persistence_diagrams[j])\n",
    "            distance = wasserstein_distance(diagram1, diagram2, order=1., internal_p=2.)\n",
    "            distances[i, j] = distance\n",
    "            distances[j, i] = distance\n",
    "    return distances\n",
    "\n",
    "\n",
    "\n",
    "def count_negative_entries_below_diagonal(matrix):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    n = len(matrix)\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            if matrix[i][j] < 0:\n",
    "                count += 1\n",
    "            total += 1\n",
    "    return count, total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the context vectors for all layers and heads for the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at onlplab/alephbert-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('onlplab/alephbert-base')\n",
    "model = BertModel.from_pretrained('onlplab/alephbert-base', output_attentions=True)\n",
    "\n",
    "# Get the number of layers and heads in the model\n",
    "num_layers = model.config.num_hidden_layers\n",
    "num_heads = model.config.num_attention_heads\n",
    "\n",
    "context = []\n",
    "for i in range(len(text)-13):\n",
    "    sentence_context = []\n",
    "    for layer in range(num_layers):\n",
    "        layer_context = []\n",
    "        for head in range(num_heads):\n",
    "            layer_context.append(compute_output_model(tokenizer, model, text[i], layer, head))\n",
    "        sentence_context.append(layer_context)\n",
    "    context.append(sentence_context)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the context vectors for the first model for `paragraph`, `layer`, and `head`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 39, 64])\n",
      "tensor([[[-0.2489, -0.1003, -0.1082,  ...,  0.3224,  0.2770,  0.2138],\n",
      "         [-0.0090, -0.0289, -0.0112,  ...,  0.0417,  0.0375,  0.0492],\n",
      "         [-0.0215, -0.0292,  0.0039,  ...,  0.0391,  0.0482,  0.0245],\n",
      "         ...,\n",
      "         [ 0.0724, -0.0199, -0.0335,  ...,  0.1791,  0.0585,  0.0575],\n",
      "         [ 0.0517, -0.0192,  0.0675,  ...,  0.0278,  0.0410, -0.0075],\n",
      "         [-0.0945, -0.0731, -0.0540,  ...,  0.3514,  0.2240,  0.2213]]],\n",
      "       grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "paragraph = len(text)-14\n",
    "layer = 11\n",
    "head = 11\n",
    "\n",
    "print(context[paragraph][layer][head].shape)\n",
    "print(context[paragraph][layer][head])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the context vectors for all layers and heads for the second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dicta-il/alephbertgimmel-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dicta-il/alephbertgimmel-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_2 = AutoTokenizer.from_pretrained('dicta-il/alephbertgimmel-base')\n",
    "model_2 = AutoModel.from_pretrained(\"dicta-il/alephbertgimmel-base\", output_attentions=True) \n",
    "\n",
    "# Get the number of layers and heads in the model\n",
    "num_layers = model_2.config.num_hidden_layers\n",
    "num_heads = model_2.config.num_attention_heads\n",
    "\n",
    "context_2 = []\n",
    "for i in range(len(text)-13):\n",
    "    sentence_context_2 = []\n",
    "    for layer in range(num_layers):\n",
    "        layer_context_2 = []\n",
    "        for head in range(num_heads):\n",
    "            layer_context_2.append(compute_output_model(tokenizer_2, model_2, text[i], layer, head))\n",
    "        sentence_context_2.append(layer_context_2)\n",
    "    context_2.append(sentence_context_2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the context vectors for the second model for `paragraph`, `layer`, and `head`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 39, 64])\n",
      "tensor([[[ 0.0121,  0.0056,  0.0036,  ..., -0.0106,  0.0054,  0.0007],\n",
      "         [ 0.0053,  0.0444, -0.0070,  ..., -0.0396, -0.0418, -0.0150],\n",
      "         [ 0.0139,  0.0409,  0.0100,  ..., -0.0144, -0.0257,  0.0045],\n",
      "         ...,\n",
      "         [ 0.0423,  0.0082,  0.0512,  ..., -0.0332,  0.0426, -0.0145],\n",
      "         [ 0.0227,  0.0100,  0.0133,  ..., -0.0279,  0.0040, -0.0100],\n",
      "         [ 0.0215,  0.0009,  0.0107,  ..., -0.0203,  0.0101, -0.0038]]],\n",
      "       grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "paragraph = len(text)-14\n",
    "layer = 11\n",
    "head = 11\n",
    "\n",
    "print(context_2[paragraph][layer][head].shape)\n",
    "print(context_2[paragraph][layer][head])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')\n",
    "\n",
    "# Get the number of layers and heads in the model\n",
    "num_layers = model.config.num_hidden_layers\n",
    "num_heads = model.config.num_attention_heads\n",
    "\n",
    "persistent_homology = []\n",
    "\n",
    "for i in range(len(text)-13):\n",
    "    sentence_persistent_homology = []\n",
    "    for layer in range(num_layers):\n",
    "        layer_persistent_homology = []\n",
    "        for head in range(num_heads):\n",
    "            layer_persistent_homology.append(compute_phrase_distances_and_homology(tokenizer, context[i][layer][head], text[i], \"לצפות בזרחת השמש\"))\n",
    "        sentence_persistent_homology.append(layer_persistent_homology)\n",
    "    persistent_homology.append(sentence_persistent_homology)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (0.0, inf)),\n",
       " (0, (0.0, 1.1262223330294407)),\n",
       " (0, (0.0, 0.1849024910531314)),\n",
       " (0, (0.0, 0.07987415386492487))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_homology[paragraph][layer][head]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_2 = AutoTokenizer.from_pretrained('dicta-il/alephbertgimmel-base')\n",
    "\n",
    "# Get the number of layers and heads in the model\n",
    "num_layers = model_2.config.num_hidden_layers\n",
    "num_heads = model_2.config.num_attention_heads\n",
    "\n",
    "persistent_homology_2 = []\n",
    "\n",
    "for i in range(len(text)-13):\n",
    "    sentence_persistent_homology_2 = []\n",
    "    for layer in range(num_layers):\n",
    "        layer_persistent_homology_2 = []\n",
    "        for head in range(num_heads):\n",
    "            layer_persistent_homology_2.append(compute_phrase_distances_and_homology(tokenizer_2, context_2[i][layer][head], text[i], \"לצפות בזרחת השמש\"))\n",
    "        sentence_persistent_homology_2.append(layer_persistent_homology_2)\n",
    "    persistent_homology_2.append(sentence_persistent_homology_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of layers and heads in the model\n",
    "num_layers = model.config.num_hidden_layers\n",
    "num_heads = model.config.num_attention_heads\n",
    "\n",
    "persistence_diagrams = []\n",
    "\n",
    "for i in range(len(text)-13):\n",
    "    sentence_persistence_diagrams = []\n",
    "    for layer in range(num_layers):\n",
    "        layer_persistence_diagrams = []\n",
    "        for head in range(num_heads):\n",
    "            layer_persistence_diagrams.append(persistent_homology[i][layer][head])\n",
    "        sentence_persistence_diagrams.append(layer_persistence_diagrams)\n",
    "    persistence_diagrams.append(sentence_persistence_diagrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of layers and heads in the model\n",
    "num_layers = model_2.config.num_hidden_layers\n",
    "num_heads = model_2.config.num_attention_heads\n",
    "\n",
    "persistence_diagrams_2 = []\n",
    "\n",
    "for i in range(len(text)-13):\n",
    "    sentence_persistence_diagrams_2 = []\n",
    "    for layer in range(num_layers):\n",
    "        layer_persistence_diagrams_2 = []\n",
    "        for head in range(num_heads):\n",
    "            layer_persistence_diagrams_2.append(persistent_homology_2[i][layer][head])\n",
    "        sentence_persistence_diagrams_2.append(layer_persistence_diagrams_2)\n",
    "    persistence_diagrams_2.append(sentence_persistence_diagrams_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of layers and heads in the model\n",
    "num_layers = model.config.num_hidden_layers\n",
    "num_heads = model.config.num_attention_heads\n",
    "\n",
    "# Initialize w_distances\n",
    "w_distances = [[None for _ in range(num_heads)] for _ in range(num_layers)]\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    for head in range(num_heads):\n",
    "        w_distances[layer][head] = compute_wasserstein_distances([persistent_homology[paragraph][layer][head] for paragraph in range(len(text)-13)])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Matrices for `layer` and `head` of the first model\n",
    "\n",
    "Here we print the Wasserstein distance matrix giving the pairwise Wasserstein distances between each pair of persistence diagrams for the phrase \"לצפות בזרחת השמש\" in each of the contexts `text[i]` (we have restricted to a particular range of `i` values due to computational limitations). Note here, we have a $7 \\times 7$ distance matrix because we have restricted to `len(text)-13` contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7)\n",
      "[[0.         0.07515095 0.0845273  0.7428543  0.03266424 0.0595901\n",
      "  0.9363218 ]\n",
      " [0.07515095 0.         0.11115083 0.72884094 0.04248671 0.13474105\n",
      "  0.98512428]\n",
      " [0.0845273  0.11115083 0.         0.76863392 0.09075016 0.09187135\n",
      "  0.87803772]\n",
      " [0.7428543  0.72884094 0.76863392 0.         0.73620099 0.79467325\n",
      "  0.30408532]\n",
      " [0.03266424 0.04248671 0.09075016 0.73620099 0.         0.09225434\n",
      "  0.96148918]\n",
      " [0.0595901  0.13474105 0.09187135 0.79467325 0.09225434 0.\n",
      "  0.93537416]\n",
      " [0.9363218  0.98512428 0.87803772 0.30408532 0.96148918 0.93537416\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(w_distances[layer][head].shape)\n",
    "print(w_distances[layer][head])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of layers and heads in the model\n",
    "num_layers = model_2.config.num_hidden_layers\n",
    "num_heads = model_2.config.num_attention_heads\n",
    "\n",
    "# Initialize w_distances\n",
    "w_distances_2 = [[None for _ in range(num_heads)] for _ in range(num_layers)]\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    for head in range(num_heads):\n",
    "        w_distances_2[layer][head] = compute_wasserstein_distances([persistent_homology_2[paragraph][layer][head] for paragraph in range(len(text)-13)])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Matrices for `layer` and `head` of the second model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7)\n",
      "[[0.         0.41156989 0.30807921 0.53128823 0.3306788  0.52056734\n",
      "  0.18950863]\n",
      " [0.41156989 0.         0.63907577 0.94285812 0.47170781 0.93213723\n",
      "  0.53128737]\n",
      " [0.30807921 0.63907577 0.         0.30378235 0.5869847  0.29306146\n",
      "  0.25166465]\n",
      " [0.53128823 0.94285812 0.30378235 0.         0.78024975 0.06063153\n",
      "  0.41157075]\n",
      " [0.3306788  0.47170781 0.5869847  0.78024975 0.         0.76952886\n",
      "  0.368679  ]\n",
      " [0.52056734 0.93213723 0.29306146 0.06063153 0.76952886 0.\n",
      "  0.4112754 ]\n",
      " [0.18950863 0.53128737 0.25166465 0.41157075 0.368679   0.4112754\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(w_distances_2[layer][head].shape)\n",
    "print(w_distances_2[layer][head])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentages of Performance\n",
    "\n",
    "This next block of code will create a $144 \\times 144$ array (or more accurately a $12 \\times 12 \\times 12 \\times 12$ array) to store the percentages indication what percentage of entries of the matrices `w_distances[layer_1][head_1] - w_distances_2[layer_2][head_2]` are negative. This effectively compares how well the persistent homology is preserved by one head in one model to how well the persistent homology is preserved a second head in a second layer of a second model. If the percentage of negative entries is high, then the matirx `w_distances_2[layer_2][head_2]` is larger in more places than not, indcating that the second model's Wasserstein distances are higher much more often than not. This means the second model is *worse* at preserving persistent homology. On the other hand, if the percentage of the entries in `w_distances[layer_1][head_1] - w_distances_2[layer_2][head_2]` then the first model's attention head outperforms the second model's attention head at preserving the persistent homology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 12, 12, 12)\n",
      "Percentages of negative entries below the diagonal:  [[[[ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     4.76190476]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [28.57142857  0.          0.         ...  4.76190476  0.\n",
      "    14.28571429]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     4.76190476]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     4.76190476]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[ 4.76190476  0.          0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.         14.28571429\n",
      "     9.52380952]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     4.76190476]\n",
      "   ...\n",
      "   [ 4.76190476  9.52380952  0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [28.57142857  0.          0.         ...  0.          0.\n",
      "    14.28571429]]\n",
      "\n",
      "  [[ 0.         14.28571429  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [19.04761905  0.          0.         ...  0.         14.28571429\n",
      "    14.28571429]\n",
      "   [ 0.          0.          0.         ...  0.          4.76190476\n",
      "     4.76190476]\n",
      "   ...\n",
      "   [ 4.76190476 14.28571429  4.76190476 ...  0.          4.76190476\n",
      "     4.76190476]\n",
      "   [ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [28.57142857  0.          0.         ...  0.          0.\n",
      "    28.57142857]]]\n",
      "\n",
      "\n",
      " [[[ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [19.04761905  0.          0.         ...  0.          4.76190476\n",
      "     4.76190476]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     9.52380952]\n",
      "   ...\n",
      "   [ 9.52380952  4.76190476  0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [28.57142857  0.          0.         ...  0.          0.\n",
      "    19.04761905]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          9.52380952  4.76190476 ...  0.          4.76190476\n",
      "     4.76190476]\n",
      "   [19.04761905  0.          0.         ...  0.         14.28571429\n",
      "     9.52380952]\n",
      "   [ 9.52380952  4.76190476  4.76190476 ...  0.          0.\n",
      "     4.76190476]\n",
      "   ...\n",
      "   [14.28571429  4.76190476  4.76190476 ...  0.          9.52380952\n",
      "     4.76190476]\n",
      "   [ 4.76190476  4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [42.85714286  0.          0.         ...  4.76190476  0.\n",
      "    23.80952381]]\n",
      "\n",
      "  [[ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     9.52380952]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]]]\n",
      "\n",
      "\n",
      " [[[14.28571429 42.85714286  4.76190476 ...  0.          4.76190476\n",
      "    19.04761905]\n",
      "   [61.9047619   4.76190476  9.52380952 ...  4.76190476 38.0952381\n",
      "    42.85714286]\n",
      "   [47.61904762 14.28571429 23.80952381 ...  9.52380952  9.52380952\n",
      "    33.33333333]\n",
      "   ...\n",
      "   [47.61904762 28.57142857  4.76190476 ...  4.76190476 33.33333333\n",
      "     4.76190476]\n",
      "   [14.28571429 28.57142857  4.76190476 ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [80.95238095  0.          0.         ... 23.80952381  0.\n",
      "    66.66666667]]\n",
      "\n",
      "  [[ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.         19.04761905\n",
      "     4.76190476]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     4.76190476]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [23.80952381  0.          0.         ...  0.          0.\n",
      "    14.28571429]]\n",
      "\n",
      "  [[ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [14.28571429  0.          0.         ...  0.          0.\n",
      "     4.76190476]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 4.76190476 19.04761905  0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [23.80952381  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [14.28571429  9.52380952  4.76190476 ...  0.          4.76190476\n",
      "     4.76190476]\n",
      "   [ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [42.85714286  0.          0.         ...  4.76190476  0.\n",
      "    19.04761905]]\n",
      "\n",
      "  [[ 4.76190476  4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          4.76190476\n",
      "     4.76190476]\n",
      "   [ 9.52380952  0.          0.         ...  4.76190476  4.76190476\n",
      "     4.76190476]\n",
      "   ...\n",
      "   [ 9.52380952  4.76190476  0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [14.28571429  0.          0.         ...  4.76190476  0.\n",
      "    14.28571429]]\n",
      "\n",
      "  [[ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [19.04761905  0.          0.         ...  0.         14.28571429\n",
      "    14.28571429]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     4.76190476]\n",
      "   ...\n",
      "   [ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [33.33333333  0.          0.         ...  0.          0.\n",
      "    19.04761905]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 0.         14.28571429  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [14.28571429  0.          0.         ...  0.         19.04761905\n",
      "     9.52380952]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 9.52380952  4.76190476  0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [42.85714286  0.          0.         ...  0.          0.\n",
      "    23.80952381]]\n",
      "\n",
      "  [[ 0.         28.57142857  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [28.57142857  0.          0.         ...  0.         23.80952381\n",
      "    14.28571429]\n",
      "   [ 9.52380952  0.          4.76190476 ...  4.76190476  4.76190476\n",
      "     9.52380952]\n",
      "   ...\n",
      "   [23.80952381 33.33333333  0.         ...  0.         14.28571429\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [52.38095238  0.          0.         ... 14.28571429  0.\n",
      "    38.0952381 ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [14.28571429  0.          0.         ...  0.          4.76190476\n",
      "     9.52380952]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [33.33333333  0.          0.         ...  0.          0.\n",
      "     9.52380952]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 4.76190476  9.52380952  0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [23.80952381  0.          0.         ...  4.76190476 23.80952381\n",
      "    23.80952381]\n",
      "   [23.80952381  0.          4.76190476 ...  0.          4.76190476\n",
      "     9.52380952]\n",
      "   ...\n",
      "   [28.57142857 14.28571429  9.52380952 ...  0.         19.04761905\n",
      "     0.        ]\n",
      "   [ 0.         14.28571429  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [42.85714286  0.          0.         ...  9.52380952  0.\n",
      "    38.0952381 ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     0.        ]]]\n",
      "\n",
      "\n",
      " [[[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          4.76190476\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     4.76190476]]\n",
      "\n",
      "  [[ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [23.80952381  0.          0.         ...  0.          0.\n",
      "     4.76190476]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 4.76190476 23.80952381  0.         ...  0.          4.76190476\n",
      "     4.76190476]\n",
      "   [28.57142857  0.          0.         ...  4.76190476  4.76190476\n",
      "     4.76190476]\n",
      "   [14.28571429  4.76190476  9.52380952 ...  4.76190476  4.76190476\n",
      "    14.28571429]\n",
      "   ...\n",
      "   [23.80952381  9.52380952  4.76190476 ...  4.76190476  0.\n",
      "     0.        ]\n",
      "   [ 4.76190476 14.28571429  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [47.61904762  0.          0.         ... 14.28571429  0.\n",
      "    33.33333333]]\n",
      "\n",
      "  [[28.57142857 23.80952381 14.28571429 ...  9.52380952 19.04761905\n",
      "    14.28571429]\n",
      "   [42.85714286 19.04761905 14.28571429 ... 14.28571429 33.33333333\n",
      "    33.33333333]\n",
      "   [28.57142857  9.52380952 19.04761905 ... 23.80952381 14.28571429\n",
      "    33.33333333]\n",
      "   ...\n",
      "   [38.0952381  28.57142857 19.04761905 ...  9.52380952 28.57142857\n",
      "    23.80952381]\n",
      "   [ 9.52380952 23.80952381  0.         ...  4.76190476  0.\n",
      "     0.        ]\n",
      "   [47.61904762  0.          0.         ... 23.80952381  0.\n",
      "    47.61904762]]\n",
      "\n",
      "  [[ 9.52380952 47.61904762  0.         ...  0.          9.52380952\n",
      "     4.76190476]\n",
      "   [57.14285714  4.76190476  4.76190476 ... 14.28571429 33.33333333\n",
      "    28.57142857]\n",
      "   [38.0952381   4.76190476 19.04761905 ...  4.76190476 14.28571429\n",
      "    33.33333333]\n",
      "   ...\n",
      "   [42.85714286 38.0952381  19.04761905 ...  9.52380952 28.57142857\n",
      "    19.04761905]\n",
      "   [ 4.76190476 38.0952381   0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [61.9047619   0.          0.         ...  9.52380952  0.\n",
      "    57.14285714]]]\n",
      "\n",
      "\n",
      " [[[ 9.52380952  9.52380952  4.76190476 ...  0.          4.76190476\n",
      "     4.76190476]\n",
      "   [23.80952381  4.76190476  4.76190476 ...  4.76190476 19.04761905\n",
      "    19.04761905]\n",
      "   [23.80952381  4.76190476  0.         ...  4.76190476  0.\n",
      "     9.52380952]\n",
      "   ...\n",
      "   [19.04761905 14.28571429  9.52380952 ...  0.         14.28571429\n",
      "     4.76190476]\n",
      "   [ 4.76190476  9.52380952  0.         ...  4.76190476  0.\n",
      "     0.        ]\n",
      "   [52.38095238  0.          0.         ...  9.52380952  0.\n",
      "    38.0952381 ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[ 0.          9.52380952  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 9.52380952  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          4.76190476  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [23.80952381  0.          0.         ...  0.          0.\n",
      "     4.76190476]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     4.76190476]]\n",
      "\n",
      "  [[ 0.         14.28571429  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [33.33333333  0.          0.         ...  0.         14.28571429\n",
      "     4.76190476]\n",
      "   [ 4.76190476  0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 9.52380952 14.28571429  0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [38.0952381   0.          0.         ...  4.76190476  0.\n",
      "    19.04761905]]\n",
      "\n",
      "  [[28.57142857 47.61904762 23.80952381 ...  9.52380952 33.33333333\n",
      "    28.57142857]\n",
      "   [52.38095238 28.57142857 33.33333333 ... 28.57142857 33.33333333\n",
      "    38.0952381 ]\n",
      "   [42.85714286 14.28571429 33.33333333 ... 38.0952381  23.80952381\n",
      "    42.85714286]\n",
      "   ...\n",
      "   [52.38095238 47.61904762 38.0952381  ... 19.04761905 38.0952381\n",
      "    38.0952381 ]\n",
      "   [19.04761905 42.85714286  9.52380952 ...  9.52380952  9.52380952\n",
      "     0.        ]\n",
      "   [66.66666667  0.          0.         ... 33.33333333  0.\n",
      "    61.9047619 ]]]]\n"
     ]
    }
   ],
   "source": [
    "# Create a 4D array to store the percentages\n",
    "percentages = np.zeros((num_layers, num_heads, num_layers, num_heads))\n",
    "\n",
    "# Loop over all pairs of layers and heads\n",
    "for layer_1 in range(num_layers):\n",
    "    for head_1 in range(num_heads):\n",
    "        for layer_2 in range(num_layers):\n",
    "            for head_2 in range(num_heads):\n",
    "                # Compute the difference of their Wasserstein distance matrices\n",
    "                matrix = w_distances[layer_1][head_1] - w_distances_2[layer_2][head_2]\n",
    "\n",
    "                # Count the percentage of negative entries below the diagonal\n",
    "                negative_count, total_count = count_negative_entries_below_diagonal(matrix)\n",
    "                percentage = (negative_count / total_count) * 100\n",
    "\n",
    "                # Store the percentage\n",
    "                percentages[layer_1, head_1, layer_2, head_2] = percentage\n",
    "\n",
    "print(percentages.shape)\n",
    "print(\"Percentages of negative entries below the diagonal: \", percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.761904761904762\n"
     ]
    }
   ],
   "source": [
    "layer_1 = 11\n",
    "head_1 = 5\n",
    "\n",
    "layer_2 = 1\n",
    "head_2 = 1\n",
    "\n",
    "print(percentages[layer_1][head_1][layer_2][head_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  0.0\n",
      "Max:  100.0\n",
      "Mean:  4.911403218694884\n",
      "Median:  0.0\n",
      "Standard Deviation:  11.477514926588672\n",
      "25th percentile:  0.0\n",
      "75th percentile:  4.761904761904762\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a list to store all entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over all entries in the 4D array\n",
    "for layer_1 in range(num_layers):\n",
    "    for head_1 in range(num_heads):\n",
    "        for layer_2 in range(num_layers):\n",
    "            for head_2 in range(num_heads):\n",
    "                # Add the entry to the list\n",
    "                all_entries.append(percentages[layer_1, head_1, layer_2, head_2])\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "all_entries = np.array(all_entries)\n",
    "\n",
    "# Compute the statistics\n",
    "min_value = np.min(all_entries)\n",
    "max_value = np.max(all_entries)\n",
    "mean_value = np.mean(all_entries)\n",
    "median_value = np.median(all_entries)\n",
    "std_dev = np.std(all_entries)\n",
    "percentile_25 = np.percentile(all_entries, 25)\n",
    "percentile_75 = np.percentile(all_entries, 75)\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Min: \", min_value)\n",
    "print(\"Max: \", max_value)\n",
    "print(\"Mean: \", mean_value)\n",
    "print(\"Median: \", median_value)\n",
    "print(\"Standard Deviation: \", std_dev)\n",
    "print(\"25th percentile: \", percentile_25)\n",
    "print(\"75th percentile: \", percentile_75)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can tell that a large majority of the time the second model `dicta-il/alephbertgimmel-base` outperforms the first model `TurkuNLP/wikibert-base-he-cased` at preserving the persistent homology of the phrase \"לצפות בזרחת השמש\" in the various contexts given by each `text[i]`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
